# -*- coding: utf-8 -*-
"""SA_sentence_transformers_all_MiniLM_L6_v2 (5).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1g-3Op3bFFb2TwvVJmxgciG-QYP_Sww5b
"""

print("SHREE RAM")

#!pip install anvil-uplink

'''import anvil.server

anvil.server.connect("server_GZMJ3FGREXONNV7RQ3PFQ6DI-BX4UIUOEXNLMKCT7")'''

'''@anvil.server.callable
def detect_sar_3(input):
  return input+" Hi from MiniLM"'''

#anvil.server.wait_forever()

pip install sentence-transformers imbalanced-learn scikit-learn pandas numpy

#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
LLaMA-style Embeddings with Random Forest Classification

This script uses LLaMA-compatible embeddings (via sentence-transformers)
with a Random Forest classifier for sentiment analysis and sarcasm detection.
"""

import os
import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from imblearn.over_sampling import SMOTE


def download_datasets():
    """Download required datasets if they don't exist."""
    datasets = [
        ("https://gist.githubusercontent.com/Phani-2608/94aa46d8f50b5ce2771a84df8053832d/raw/1cda172f2ccd6e7cf7b6eee2c9e1e2cadf2c4fd5/datasetSentences.csv", "datasetSentences.csv"),
        ("https://gist.githubusercontent.com/Phani-2608/054494172f6ccc8f238a5df5d62f40f4/raw/356db4dfc913f89c54741976bad08ecb2b2fa55d/datasetLabels.csv", "datasetLabels.csv"),
        ("https://huggingface.co/datasets/Ziyuan111/sarcasm/raw/main/combined_data.csv",
         "combined_data.csv")
    ]

    for url, filename in datasets:
        if not os.path.exists(filename):
            print(f"Downloading {filename}...")
            os.system(f"wget {url}")


def load_data():
    df = pd.read_csv("combined_data.csv")
    # rename/align columns
    df = df[['comments', 'contains_slash_s']].rename(
        columns={'comments': 'sentence',
                 'contains_slash_s': 'label'}
    )
    # force string and drop any missing
    df['sentence'] = df['sentence'].astype(str)
    df = df.dropna(subset=['sentence', 'label'])
    return df

def get_bert_features(text, model):
    text = str(text)
    emb = model.encode([text], convert_to_numpy=True)
    return emb[0]





def process_data(df, model):
    print("Extracting sentence embeddings...")
    return np.vstack([
        get_bert_features(text, model)
        for text in df['sentence']
    ])


def apply_smote(X, y):
    #attempts to correct imbalances in the data using smote
    """Apply SMOTE to handle class imbalance."""
    print("Applying SMOTE to balance classes...")

    #autobalances the data and using predetermined seed
    smote = SMOTE(sampling_strategy='auto', random_state=42)
    X_resampled, y_resampled = smote.fit_resample(X, y)

    #returns resampled data
    return X_resampled, y_resampled


def train_and_evaluate_model(X_train, X_test, y_train, y_test):
    """Train Random Forest model and evaluate its performance."""
    print("Training Random Forest classifier...")

    #random forest classifier used
    #needed to prevent overfitting and better accuracy
    clf = RandomForestClassifier(
        n_estimators=100,
        random_state=42,
        n_jobs=-1,
        verbose=1
    )

    #data is then fed to tree and learned
    clf.fit(X_train, y_train)

    #predictions are then scored
    predictions = clf.predict(X_test)
    accuracy = accuracy_score(y_test, predictions)
    report = classification_report(y_test, predictions)
    print(f"Accuracy: {accuracy:.4f}")
    print("Classification Report:\n", report)

    return clf, accuracy, report


def main():

    #model loaded in and used
    """Main function to orchestrate the entire process."""
    print("Loading MiniLM-style sentence embedding model...")
    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')  # Replace with LLaMA variant if available
    tokenizer = None  # Not used in sentence-transformers

    download_datasets()

    #data information printed
    df = load_data()
    print(f"Dataset shape: {df.shape}")
    print(f"Class distribution:\n{df['label'].value_counts()}")

    #data tokenized and then resampled
    X = process_data(df, model)
    y = df['label'].values

    #resampling then testing and training data created
    X_resampled, y_resampled = apply_smote(X, y)
    print(f"Shape after SMOTE: X={X_resampled.shape}, y={y_resampled.shape}")
    X_train, X_test, y_train, y_test = train_test_split(
        X_resampled, y_resampled, test_size=0.2, random_state=42
    )

    #information then printed
    model, accuracy, report = train_and_evaluate_model(X_train, X_test, y_train, y_test)
    print("Process completed successfully!")
    return model


if __name__ == "__main__":
    model = main()

query = 'Ride a bike'
query_emb = get_bert_features(query, SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2'))
query_emb[:10]

def predict_sarcasm(query):

  query_emb = get_bert_features(query, SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2'))
  res = model.predict(query_emb.reshape(1,-1))[0]
  if res:
    return 'The query is sarcastic'
  else:
    return 'the query is not sarcastic'

query = 'ryxxdtydty/s ftyftty'
predict_sarcasm(query)

